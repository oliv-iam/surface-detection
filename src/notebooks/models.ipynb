{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5449e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/c/Users/olivi/OneDrive - Amherst College/6 Summer 2025/surface-detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12dd00",
   "metadata": {},
   "source": [
    "##### Confusion heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d989d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix heatmap, normalized over true (rows)\n",
    "def confusion_heatmap(model=\"ver6\", fsetname=\"train\", setname=\"Training\", dataset=1):\n",
    "    # set= pd.read_csv(f\"{path}/logs/set-results/{model}_accuracy_{fsetname}_set{dataset}.txt\")\n",
    "    # set = pd.read_csv(f\"{path}/logs/set-results/set{dataset}_{fsetname}.txt\")\n",
    "    set = pd.read_csv(f\"{path}/logs/set-results/set{dataset}_{fsetname}_concat.txt\")\n",
    "    sklearn.metrics.ConfusionMatrixDisplay.from_predictions(set[\"True\"], set[\"Predicted\"], \n",
    "                                                            # cmap='cividis', \n",
    "                                                            cmap = 'BuPu',\n",
    "                                                            display_labels=[\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "                                                            normalize=\"true\")\n",
    "    plt.xlabel(\"Predicted location\")\n",
    "    plt.ylabel(\"True location\")\n",
    "    plt.title(f\"{model.capitalize()} Predictions on {setname.capitalize()} Set (Set {dataset})\")\n",
    "    plt.savefig(f\"{path}/figures/set-results/set{dataset}_confusion_{fsetname}.png\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ea654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix heatmap, normalized over true (rows)\n",
    "def accuracy(model=\"ver6\", fsetname=\"train\", setname=\"Training\", dataset=1):\n",
    "    set = pd.read_csv(f\"{path}/logs/set-results/set{dataset}_{fsetname}_concat.txt\")\n",
    "    confusion = sklearn.metrics.confusion_matrix(set[\"True\"], set[\"Predicted\"])\n",
    "    print(np.trace(confusion) / np.sum(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set = 9\n",
    "model = \"\"\n",
    "accuracy(model, \"train\", \"training\", set)\n",
    "# confusion_heatmap(model, \"val\", \"validation\", set)\n",
    "accuracy(model, \"test\", \"test\", set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e41cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "set = 9\n",
    "model = \"basic\"\n",
    "confusion_heatmap(model, \"train\", \"training\", set)\n",
    "# confusion_heatmap(model, \"val\", \"validation\", set)\n",
    "confusion_heatmap(model, \"test\", \"test\", set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed3590",
   "metadata": {},
   "source": [
    "##### Accuracy, Precision and Recall Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = [\"ver2\", \"ver3\", \"ver4\", \"ver5a\", \"ver5b\", \"ver6\", \"ver7\"]\n",
    "sets = [\"train\", \"val\", \"test\"]\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\"]\n",
    "trials = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(metric=\"accuracy\", trial=1):\n",
    "    x = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\n",
    "    for k in range(len(versions)):\n",
    "        model = versions[k]\n",
    "        y = []\n",
    "        if model == \"ver2\" and trial == 1:\n",
    "            continue\n",
    "        for i in range(len(sets)):\n",
    "            guesses = pd.read_csv(f\"{path}/logs/nn-results/{model}_accuracy_{sets[i]}_{trial}.txt\")\n",
    "            confusion = sklearn.metrics.confusion_matrix(guesses[\"True\"], guesses[\"Predicted\"])\n",
    "            if metric == \"precision\":\n",
    "                # precision = TP / (TP + FP)\n",
    "                y.append([confusion[j][j] / np.sum(confusion[:, j]) for j in range(len(x))])\n",
    "            elif metric == \"recall\":\n",
    "                # recall = TP / (TP + FN)\n",
    "                y.append([confusion[j][j] / np.sum(confusion[j]) for j in range(len(x))])\n",
    "            else:\n",
    "                # accuracy = true predictions / all predictions\n",
    "                y.append([np.trace(confusion) / np.sum(confusion) for j in range(len(x))])\n",
    "\n",
    "        colors = plt.get_cmap('Set2').colors\n",
    "\n",
    "        ax1.plot(x, y[0], label=model, color=colors[k])\n",
    "        ax2.plot(x, y[1], label=model, color=colors[k])\n",
    "        ax3.plot(x, y[2], label=model, color=colors[k])\n",
    "\n",
    "        if trial == 1:\n",
    "            trialname = \"Even\"\n",
    "        else:\n",
    "            trialname = \"U5 Separate\"\n",
    "    \n",
    "    ax1.set_ylabel(metric.capitalize())\n",
    "    ax2.set_xlabel(\"Location\")\n",
    "    ax2.set_title(f\"{metric.capitalize()} on Training, Validation, and Test Sets ({trialname})\")\n",
    "    ax3.legend(loc='center right', bbox_to_anchor=(1.34, 0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{path}/figures/nn-results/{metric}_{trial}.png\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison(\"accuracy\", 1)\n",
    "comparison(\"accuracy\", 2)\n",
    "comparison(\"recall\", 1)\n",
    "comparison(\"recall\", 2)\n",
    "comparison(\"precision\", 1)\n",
    "comparison(\"precision\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d56e0",
   "metadata": {},
   "source": [
    "##### Recall vs. Class Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_counts = [729, 660, 747, 721, 675]\n",
    "versions = [\"ver2\", \"ver3\", \"ver4\", \"ver5a\", \"ver5b\", \"ver6\", \"ver7\"]\n",
    "sets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# count true positives across versions\n",
    "tps = [[0, 0, 0, 0, 0] for i in range(len(sets))]\n",
    "for i in range(len(versions)):\n",
    "    model = versions[i]\n",
    "    for j in range(len(sets)):\n",
    "        guesses = pd.read_csv(f\"{path}/logs/nn-results/{model}_accuracy_{sets[j]}_2.txt\")\n",
    "        confusion = sklearn.metrics.confusion_matrix(guesses[\"True\"], guesses[\"Predicted\"], normalize=\"true\")\n",
    "        for k in range(5):\n",
    "            tps[j][k] += confusion[k][k]\n",
    "\n",
    "# calculate average and recall (TP / (TP + FN))\n",
    "for i in range(len(sets)):\n",
    "    for j in range(5):\n",
    "        tps[i][j] = (tps[i][j] / len(versions))\n",
    "\n",
    "# plot\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax1.scatter(s1_counts, tps[0])\n",
    "ax2.scatter(s1_counts, tps[1])\n",
    "ax3.scatter(s1_counts, tps[2])\n",
    "\n",
    "# labels\n",
    "ax1.set_title(\"Training Set\")\n",
    "ax2.set_title(\"Validation Set\")\n",
    "ax3.set_title(\"Test Set\")\n",
    "ax1.set_ylabel(\"Recall\")\n",
    "ax2.set_xlabel(\"Items in Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0c28c",
   "metadata": {},
   "source": [
    "##### Violin plot: accuracy vs. dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c5b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [\"set1\", \"set2\", \"set3\", \"set4\", \"set5\", \"set6\"]\n",
    "datasets = [\"train\", \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin1(set):\n",
    "    arr = np.zeros((10,2))\n",
    "    for i in range(len(datasets)):\n",
    "        for j in range(10):\n",
    "            preds = pd.read_csv(f\"{path}/logs/set-results/{set}_{datasets[i]}_{j+1}.txt\")\n",
    "            confusion = sklearn.metrics.confusion_matrix(preds[\"True\"], preds[\"Predicted\"])\n",
    "            arr[j,i] = np.trace(confusion) / np.sum(confusion)\n",
    "    plt.violinplot(arr, showmedians=True)\n",
    "    plt.title(f\"Accuracy over 10 Trials ({set.capitalize()})\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Dataset (Train, Test = 1, 2)\")\n",
    "    plt.savefig(f\"{path}/figures/set-results/{set}_violin.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin2():\n",
    "    arr1 = np.zeros((10, len(sets)))\n",
    "    arr2 = np.zeros((10,len(sets)))\n",
    "    for i in range(len(sets)):\n",
    "        for j in range(10):\n",
    "            # training set\n",
    "            preds1 = pd.read_csv(f\"{path}/logs/set-results/{sets[i]}_train_{j+1}.txt\")\n",
    "            confusion1 = sklearn.metrics.confusion_matrix(preds1[\"True\"], preds1[\"Predicted\"])\n",
    "            arr1[j,i] = np.trace(confusion1) / np.sum(confusion1)\n",
    "\n",
    "            # test set\n",
    "            preds2 = pd.read_csv(f\"{path}/logs/set-results/{sets[i]}_test_{j+1}.txt\")\n",
    "            confusion2 = sklearn.metrics.confusion_matrix(preds2[\"True\"], preds2[\"Predicted\"])\n",
    "            arr2[j,i] = np.trace(confusion2) / np.sum(confusion2)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n",
    "    \n",
    "    plots1 = ax1.violinplot(arr1, showmedians=True)\n",
    "    # plots1['bodies'][-1].set_facecolor(\"purple\")\n",
    "    ax1.set_title(\"Training Set\", fontsize=10)\n",
    "    ax1.set_xlabel(\"Dataset Version\")\n",
    "    ax1.set_xlabel(\"Dataset Version\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "\n",
    "    plots2 = ax2.violinplot(arr2, showmedians=True)\n",
    "    # plots2['bodies'][-1].set_facecolor(\"purple\")\n",
    "    ax2.set_title(\"Test Set\", fontsize=10)\n",
    "    ax2.set_xlabel(\"Dataset Version\")\n",
    "\n",
    "    plt.suptitle(\"2 Channels (1) vs. 6 Channels (2)\")\n",
    "\n",
    "    # plt.title(f\"Accuracy over 10 Trials\")\n",
    "    # plt.savefig(f\"{path}/figures/set-results/{set}_violin.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "violin1(\"set9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets = [\"set1\", \"set4\", \"set5\", \"set7\"]\n",
    "sets = [\"set1\", \"set9\"]\n",
    "# sets = [\"set1\", \"set7\", \"set9\"]\n",
    "violin2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surf_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
